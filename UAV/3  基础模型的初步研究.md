# 3  基础模型的初步研究

本节概述了基础模型，包括 LLMs、视觉基础模型（VFMs）和视觉语言模型（VLMs）。它强调了它们的核心特征和技术优势，旨在为这些模型与无人机系统的深度集成提供基础见解和指导。

表 2: LLMs、VLMs 和 VFMs 的总结。

|**类别**|**子类别**|**模型名称**|**机构 / 作者**|
| ------| ----------| ----------------------------------------------| -------|
|LLMs|一般|GPT-3[[141]]，GPT-3.5[[142]]，GPT-4[[143]]|[OpenAI](https://openai.com/)|
|||Claude 2，Claude 3[[144]，[145]，[146]]|[人类本位主义](https://www.anthropic.com/)|
|||Mistral 系列[[147]，[148]]|[Mistral AI](https://www.mistral.ai/)|
|||PaLM 系列[[149],[150]]，Gemini 系列[[151],[152]]|[Google 研究](https://ai.google/)|
|||LLaMA[[153]]，LLaMA2[[154]]，LLaMA3[[155]]|[Meta AI](https://ai.meta.com/)|
|||Vicuna[[156]]|[Vicuna 团队](https://vicuna.lmsys.org/)|
|||Qwen 系列[[157],[158]]|[Qwen 团队，阿里巴巴集团](https://github.com/QwenLM)|
|||InternLM[[159]]|[上海人工智能实验室](https://github.com/InternLM/InternLM)|
|||BuboGPT[[160]]|[字节跳动](https://github.com/magic-research/bubogpt)|
|||ChatGLM[[161],[162],[163]]|[Zhipu AI](https://github.com/THUDM)|
|||DeepSeek 系列[[164],[165],[166]]|[DeepSeek](https://github.com/deepseek-ai)|
|VLMs|一般|GPT-4V[[167]], GPT-4o, GPT-4o 迷你版, GPT o1 预览|[OpenAI](https://openai.com/)|
|||Claude 3 Opus, Claude 3.5 Sonnet[[168]]|[人类本位主义](https://www.anthropic.com/)|
|||步骤-2|[解悦星辰](https://www.stepfun.com/)|
|||LLaVA[[169]], LLaVA-1.5[[170]], LLaVA-NeXT[[171]]|[刘](https://github.com/haotian-liu/LLaVA) *[等](https://github.com/haotian-liu/LLaVA)* [.](https://github.com/haotian-liu/LLaVA)|
|||MoE-LLaVA[[172]]|[林](https://github.com/PKU-YuanGroup/MoE-LLaVA) *[等](https://github.com/PKU-YuanGroup/MoE-LLaVA)* [.](https://github.com/PKU-YuanGroup/MoE-LLaVA)|
|||LLaVA-CoT[[173]]|[徐](https://github.com/PKU-YuanGroup/LLaVA-CoT)​*[等](https://github.com/PKU-YuanGroup/LLaVA-CoT)* [。](https://github.com/PKU-YuanGroup/LLaVA-CoT)|
|||Flamingo[[174]]|[Alayrac](https://github.com/mlfoundations/open_flamingo) *[等](https://github.com/mlfoundations/open_flamingo)* [。](https://github.com/mlfoundations/open_flamingo)|
|||BLIP[[175]]|[李](https://github.com/salesforce/BLIP) *[等](https://github.com/salesforce/BLIP)* [.](https://github.com/salesforce/BLIP)|
|||BLIP-2[[176]]|[李](https://github.com/salesforce/LAVIS/tree/main/projects/blip2) *[等](https://github.com/salesforce/LAVIS/tree/main/projects/blip2)* [.](https://github.com/salesforce/LAVIS/tree/main/projects/blip2)|
|||InstructBLIP[[177]]|[Dai](https://github.com/salesforce/LAVIS/tree/main/projects/instructblip) *[等](https://github.com/salesforce/LAVIS/tree/main/projects/instructblip)* [.](https://github.com/salesforce/LAVIS/tree/main/projects/instructblip)|
||视频理解|LLaMA-VID[[178]]|[Li](https://github.com/dvlab-research/LLaMA-VID) *[等](https://github.com/dvlab-research/LLaMA-VID)* [.](https://github.com/dvlab-research/LLaMA-VID)|
|||IG-VLM【[179]】|[金](https://github.com/imagegridworth/IG-VLM) *[等](https://github.com/imagegridworth/IG-VLM)* [。](https://github.com/imagegridworth/IG-VLM)|
|||视频-聊天生成预训练变换器[[180]]|[马兹](https://github.com/mbzuai-oryx/Video-ChatGPT) *[等](https://github.com/mbzuai-oryx/Video-ChatGPT)* [。](https://github.com/mbzuai-oryx/Video-ChatGPT)|
|||视频树[[181]]|[王](https://github.com/Ziyang412/VideoTree) *[等](https://github.com/Ziyang412/VideoTree)* [。](https://github.com/Ziyang412/VideoTree)|
||视觉推理|X-VLM[[182]]|[曾](https://github.com/zengyan-97/X-VLM) *[等](https://github.com/zengyan-97/X-VLM)* [。](https://github.com/zengyan-97/X-VLM)|
|||变色龙[[183]]|[卢](https://chameleon-llm.github.io/) *[等](https://chameleon-llm.github.io/)* [。](https://chameleon-llm.github.io/)|
|||HYDRA[[184]]|[柯](https://hydra-vl4ai.github.io/) *[等](https://hydra-vl4ai.github.io/)* [。](https://hydra-vl4ai.github.io/)|
|||VISPROG[[185]]|[优先 @ 艾伦人工智能研究所](https://prior.allenai.org/projects/visprog)|
|VFMs|一般|CLIP[[186]]|[OpenAI](https://github.com/OpenAI/CLIP)|
|||FILIP[[187]]|姚等.|
|||RegionCLIP[[188]]|[微软研究院](https://github.com/microsoft/RegionCLIP)|
|||EVA-CLIP[[189]]|[Sun](https://github.com/baaivision/EVA/tree/master/EVA-CLIP) *[等](https://github.com/baaivision/EVA/tree/master/EVA-CLIP)* [。](https://github.com/baaivision/EVA/tree/master/EVA-CLIP)|
||目标检测|GLIP[[190]]|[微软研究院](https://github.com/microsoft/GLIP)|
|||DINO[[191]]|[张](https://github.com/IDEA-Research/DINO)​*[等](https://github.com/IDEA-Research/DINO)* [。](https://github.com/IDEA-Research/DINO)|
|||Grounding-DINO[[192]]|[刘](https://github.com/IDEA-Research/GroundingDINO)​*[等](https://github.com/IDEA-Research/GroundingDINO)* [。](https://github.com/IDEA-Research/GroundingDINO)|
|||DINOv2[[193]]|[Meta AI 研究](https://github.com/facebookresearch/dinov2)|
|||AM-RADIO[[194]]|[NVIDIA](https://github.com/NVlabs/RADIO)|
|||DINO-WM[[195]]|[周](https://dino-wm.github.io/) *[等](https://dino-wm.github.io/)* [.](https://dino-wm.github.io/)|
|||YOLO-世界[[196]]|[Cheng](https://github.com/AILabCVC/YOLO-World) *[等](https://github.com/AILabCVC/YOLO-World)* [.](https://github.com/AILabCVC/YOLO-World)|
||图像分割|CLIPSeg[[197]]|[Lüdecke 和 Ecker](https://github.com/timojl/clipseg)|
|||SAM[[198]]|[Meta AI 研究，FAIR](https://segment-anything.com/)|
|||具身-SAM[[199]]|[Xu](https://github.com/xuxw98/ESAM) *[等](https://github.com/xuxw98/ESAM)* [。](https://github.com/xuxw98/ESAM)|
|||点状 SAM[[200]]|[Zhou](https://github.com/zyc00/Point-SAM) *[等](https://github.com/zyc00/Point-SAM)* [.](https://github.com/zyc00/Point-SAM)|
|||开放词汇 SAM[[201]]|[Yuan](https://www.mmlab-ntu.com/project/ovsam/) *[等](https://www.mmlab-ntu.com/project/ovsam/)* [.](https://www.mmlab-ntu.com/project/ovsam/)|
|||TAP[[202]]|[潘](https://github.com/baaivision/tokenize-anything) *[等](https://github.com/baaivision/tokenize-anything)* [。](https://github.com/baaivision/tokenize-anything)|
|||EfficientSAM[[203]]|[熊](https://yformer.github.io/efficient-sam/) *[等](https://yformer.github.io/efficient-sam/)* [。](https://yformer.github.io/efficient-sam/)|
|||移动 SAM[[204]]|[张](https://github.com/ChaoningZhang/MobileSAM) *[等](https://github.com/ChaoningZhang/MobileSAM)* [.](https://github.com/ChaoningZhang/MobileSAM)|
|||SAM 2[[205]]|[Meta AI 研究，FAIR](https://ai.meta.com/sam2/)|
|||SAMURAI[[206]]|[华盛顿大学](https://github.com/yangchris11/samurai)|
|||SegGPT[[207]]|[王](https://github.com/baaivision/Painter)​*[等](https://github.com/baaivision/Painter)* [。](https://github.com/baaivision/Painter)|
|||鱼鹰[[208]]|[袁](https://github.com/CircleRadon/Osprey)​*[等](https://github.com/CircleRadon/Osprey)* [。](https://github.com/CircleRadon/Osprey)|
|||似乎[[209]]|[Zou](https://github.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once) *[等](https://github.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once)* [.](https://github.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once)|
|||Seal[[210]]|[Liu](https://github.com/youquanl/Segment-Any-Point-Cloud) *[等](https://github.com/youquanl/Segment-Any-Point-Cloud)* [.](https://github.com/youquanl/Segment-Any-Point-Cloud)|
|||LISA[[211]]|[赖](https://github.com/dvlabresearch/LISA) *[等](https://github.com/dvlabresearch/LISA)* [.](https://github.com/dvlabresearch/LISA)|
||深度估计|ZoeDepth[[212]]|[巴特](https://github.com/isl-org/ZoeDepth) *[等](https://github.com/isl-org/ZoeDepth)* [.](https://github.com/isl-org/ZoeDepth)|
|||尺度深度[[213]]|[Zhu](https://ruijiezhu94.github.io/ScaleDepth/) *[等](https://ruijiezhu94.github.io/ScaleDepth/)* [.](https://ruijiezhu94.github.io/ScaleDepth/)|
|||深度任何事物[[214]]|[Yang](https://depth-anything.github.io/) *[等](https://depth-anything.github.io/)* [.](https://depth-anything.github.io/)|
|||深度万物 V2[[215]]|[杨](https://depth-anything-v2.github.io/) *[等](https://depth-anything-v2.github.io/)* [.](https://depth-anything-v2.github.io/)|
|||深度专业[[216]]|[苹果](https://github.com/apple/ml-depth-pro)|

### 3.1 **LLMs**

近年来，LLMs 迅速发展，越来越大规模的模型在多样化的大规模语料库上进行训练 [[217]]。这些模型在各种自然语言处理任务中持续设定新的性能基准，并在学术研究和工业应用中得到广泛采用 [[218], [219], [220], [221]]。本节概述了 LLMs 的核心能力，包括它们的泛化和推理能力，随后介绍了来自领先研究机构的典型 LLMs。

#### 3.1.1 LLMs 的核心能力

**概括能力：**  得益于在大规模语料库上的训练以及模型的巨大规模，LLMs 展现出强大的迁移能力，包括零样本学习和少样本学习。这些能力使得 LLMs 能够有效地推广到新任务，无需特定任务的示例或仅需有限的指导，从而成为各种应用的多功能工具。在零样本学习中，LLMs 可以仅通过自然语言指令解决相关问题，无需额外的特定任务训练。在少样本学习中，模型可以通过利用支持集中的几个示例及相应的任务指令快速适应新任务 [[222]]。

自然语言指令或提示的设计在增强泛化能力方面至关重要。提示不仅提供了任务的自然语言描述，还指导模型根据输入示例准确执行任务 [[223], [141], [143]]。此外，LLMs 具有上下文学习能力，使它们能够直接从提示中提供的上下文（例如任务说明和示例）学习和适应新任务，而无需任何明确的再训练或模型更新 [[141], [224], [225]]。

**复杂问题解决能力：** LLMs 展现出解决复杂问题的卓越能力，能够生成中间推理步骤或结构化逻辑路径，从而促进以系统和逐步的方式应对挑战。这一能力通过思维链（CoT）框架得以体现，其中复杂问题被分解为一系列可管理的子任务，每个子任务使用逐步推理的示例依次解决 [[226], [227], [228], [229]]。此外，LLMs 在任务规划和工具协调方面还表现出先进的能力，使其能够调用适当的资源以满足特定子任务的要求，并有效整合工作流程以实现全面解决方案 [[230], [231], [232]]。

#### 3.1.2  典型的 LLMs

多个显著的里程碑标志着 LLMs 的发展。OpenAI 的 GPT 系列，包括 GPT-3、GPT-3.5 和 GPT-4，通过利用广泛的参数和优化的架构，在语言理解、生成和推理任务中设定了基准 [[141], [142], [143]]。Anthropic 的 Claude 模型，包括 Claude 2 和 Claude 3，优先考虑安全性和可控性，通过强化学习，在多任务泛化和鲁棒性方面表现出色 [[144], [145], [146]]。Mistral 系列采用稀疏激活技术，以在效率与性能之间取得平衡，强调低延迟推理 [[147], [148]]。

谷歌的 PaLM 系列 [[149], [150]] 因其多模态能力和大规模参数化而脱颖而出，而后续的 Gemini 系列则扩展了这些特性，以提高泛化能力和多语言支持 [[151], [152]]。在开源生态系统中，Meta 的 Llama 模型，包括 Llama 2 和 Llama 3，在多语言任务和复杂问题解决方面表现出色。派生模型如 Vicuna 通过在对话数据集上的微调和低秩适应（LoRA）等技术增强了对话能力和任务适应性 [[153], [154], [155], [156]]。同样，Qwen 系列经过多语言数据集的预训练和指令调整，在各种任务中展现了适应性 [[157]]。

在专门领域中，其他几个模型已取得显著进展。InternLM [[159]]、BuboGPT [[160]]、ChatGLM [[161], [162], [163]] 和 DeepSeek [[164], [165], [166]] 专注于基于知识的问答、对话生成和信息检索等特定领域任务，通过特定任务的微调和针对性的扩展来实现。值得注意的是，LiveBench [[233]] 已成为一个综合性基准测试平台，解决了之前评估标准的局限性。它系统地评估了 LLMs 在多任务场景下的实际能力，为模型开发与应用提供了宝贵的见解。

### 3.2 **VLMs**

VLMs 是多模态模型，通过整合视觉和文本信息扩展了 LLMs 的能力 [[234]]。这些模型旨在解决需要视觉和语言理解的一系列任务，比如视觉问答 (VQA) 和图像标注 [[235], [236], [237], [238], [239]]。本节介绍了几种典型的 VLM 模型，重点介绍它们的技术特点和应用场景。

OpenAI 的 GPT-4V [[167]] 是视觉语言模型 (VLMs) 中的一个重要代表，展现了强大的视觉感知能力 [[240]]。升级版的 GPT-4o 引入了更先进的优化算法，使其能够接受任意组合的文本、音频和图像输入，同时提供快速响应。轻量级版本 GPT-4o mini 专为移动设备和边缘计算场景设计，通过减少计算资源消耗，平衡高效性能与可部署性 [[241]]。GPT o1-preview 在推理方面表现出色，尤其是在编程和解决复杂问题上 [[242]]。Anthropic 的 Claude 3 Opus 展现了强大的多任务泛化和可控性，而 Claude 3.5 Sonnet 通过优化推理速度和成本效率提升了实际价值 [[168]]。Step-2 模型采用了创新的专家混合 (Mixture of Experts, MoE) 架构，支持在万亿参数规模上进行高效训练，并显著改善复杂任务的处理和模型的可扩展性。

刘*等*  [[169]] 提出了 LLaVA，这是一种代表性的视觉语言模型（VLM）。该模型利用 GPT-4 生成遵循指令的数据集，并集成了 CLIP 视觉编码器 ViT-L/14 [[186]] 与 Vicuna [[243]]，通过端到端的微调提升其在多模态任务中的表现。其最新版本 LLaVA-NeXT [[171]] 在 LLaVA-1.5 [[170]] 的基础上进行了显著改进，特别是增强了捕捉视觉细节的能力，并在复杂的视觉和逻辑推理任务中表现突出。MoE-LLaVA 用 MoE 架构替代 LLaVA 中的语言模型，显著提高了大型多任务场景下的推理效率和资源利用率 [[172]]。LLaVA-CoT 通过对大规模视觉问答样本的结构化推理注释与束搜索方法结合，提升了在推理密集任务中的准确性 [[173]]。 另一个重要的架构类别包括 Flamingo [[174]] 和 BLIP 系列 [[175], [176]]，它们通过将预训练的视觉特征编码器与预训练的 LLMs 相结合，使得 LLMs 能够从多模态输入生成相应的文本输出。Flamingo 引入了感知重采样器和门控跨注意力机制，有效地将视觉、多模态信息与语言模型集成，从而显著提高了多模态任务的性能。BLIP-2 [[176]] 采用了一种结合逐阶段冻结图像编码器与 LLMs 的预训练策略，并引入了查询变换器（Q-Former），有效解决了视觉和语言模态之间的对齐问题。InstructBLIP [[177]] 结合了大规模任务指导微调机制，进一步提高了模型对多模态任务的适应性。

此外，视觉语言模型（VLMs）在各种任务和场景中展示了广泛的应用潜力。在视频理解方面，代表性模型如 LLaMA-VID [[178]]、IG-VLM [[179]]、Video-ChatGPT [[180]] 和 VideoTree [[181]] 在视频内容分析和多模态任务中表现出色。在视觉推理方面，X-VLM [[182]]、Chameleon [[183]]、HYDRA [[184]] 和 VISPROG [[185]] 通过创新的架构设计和推理机制提高了复杂视觉推理任务的准确性和适应性。

### 3.3  视觉特征模型（VFM）

近年来，视觉特征模型（VFM）作为计算机视觉中的核心技术概念应运而生。VFM 的主要目标是提取多样且高度表达性的图像特征，使其能够直接应用于各种下游任务。这些模型通常以大规模参数、显著的泛化能力和出色的跨任务迁移性能为特征，尽管其训练成本相对较高 [[194]]。CLIP 是 VFM 领域的一个开创性代表。通过对大规模图像-文本对进行弱监督训练，它高效地对齐了视觉和文本嵌入，为多模态学习打下了坚实的基础 [[186]]。后续的研究进一步提高了 CLIP 的训练效率和性能，包括 FILIP [[187]]、RegionCLIP [[188]] 和 EVA-CLIP [[189]] 等模型。

VFMs 展现了卓越的适应性，在多种计算机视觉任务中取得了显著成果，包括零-shot 目标检测、图像分割和深度估计。如图 [3] 所示，我们从 SynDrone 数据集中 Town10HD 场景中选择了一张样本图像 [[244]]，该数据集特定于无人机领域，以直观展示多个 VFMs 在零-shot 条件下的性能。这个例子为理解它们的实际应用潜力提供了有力支持。

![Refer to caption](assets/1-20250926154951-r0w6buf.png "图 3： 在各种视觉任务中 VFM 模型的演示。 (a) 来自 SynDrone 数据集的原始图像 [244]； (b) 使用 Grounding DINO 进行的目标检测结果 [192]，检测目标为自然语言提示“车”； (c) 使用 SAM 模型 [198] 对整张图像进行的语义分割； (d) 使用 ZoeDepth 模型 [212] 为整张图像生成的深度图像。")

#### 3.3.1  用于目标检测的 VFM

VFM 在目标检测中的核心优势在于其强大的零-shot 检测能力。GLIP [[190]] 统一了目标检测和短语定位任务，在各种对象级识别任务中展现出卓越的零-shot 和少-shot 迁移能力。Zhang ​ *等* ​. [[191]] 提出了 DINO，优化了 DETR 模型的架构 [[245]]，显著提升了检测性能和效率。后续工作 Grounding-DINO [[192]] 引入了文本监督以提高准确性。此外，DINOv2 [[193]] 采用了判别自监督学习的方法，使得能够提取强健的图像特征，并在下游任务中实现了优异的性能，而无需微调。 AM-RADIO [[194]] 通过多教师蒸馏方法整合了 CLIP [[186]]、DINOv2 [[193]] 和 SAM [[198]] 等 VFMs 的能力，从而产生了强大的表示能力，以支持复杂的视觉任务。DINO-WM [[195]] 将 DINOv2 集成到世界模型中，使零 shot 规划能力成为可能。此外，YOLO-World [[196]] 通过高效的预训练方案增强了 YOLO 检测器的泛化能力，在开放词汇和零 shot 检测任务中取得了卓越的性能。

#### 3.3.2  图像分割的变分自由能

VFMs 在图像分割任务中相较于传统方法展现了显著的改进。Lüdecke  *等*  [[197]] 提出了基于 CLIP 模型的 CLIPSeg，该模型支持语义分割、实例分割和零样本分割。Kirillov  *等*  [[198]] 开发了 Segment Anything Model (SAM)，通过在大规模和多样化数据集上进行预训练，实现了在多种场景下的零样本分割能力。后续研究进一步扩展了 SAM 的应用，如 Embodied-SAM [[199]] 和 Point-SAM [[200]]，扩展了 SAM 在三维场景中的功能。Open-Vocabulary SAM [[201]] 将 SAM 与 CLIP 的知识迁移策略相结合，有效地优化了分割和识别任务。Pan ​ *等* ​。 [[202]] 提出了 TAP（Tokenize Anything），一个以视觉感知为中心的基础模型，通过引入视觉提示来改善 SAM 架构，从而能够同时完成任意区域的分割、识别和描述任务。EfficientSAM [[203]] 和 MobileSAM [[204]] 优化了 SAM 的表示，显著降低了模型复杂性，并在保持出色任务性能的同时实现了轻量化设计。最近，SAM 2 [[205]] 将内存模块引入到原始模型中，使得能够对任意长度的视频进行实时分割，同时解决了遮挡和多目标跟踪等复杂挑战。SAMURAI [[206]] 在 SAM 2 的基础上构建，通过集成卡尔曼滤波器，解决了 SAM 2 中内存管理的局限性，并实现了优越的视频分割性能，无需重训练或微调。

超越 SAM 系列，其他 VFM 架构也显著推进了图像分割。诸如 SegGPT [[207]]、Osprey [[208]] 和 SEEM [[209]] 等模型在任意分割任务和多模态场景中表现出了显著的适应性。此外，VFM 在其他分割任务中也显示出了重要的应用。例如，Liu  *等*  [[210]] 提出了 Seal 框架，用于对点云序列进行分割，而 LISA [[211]] 则采用了 Embedding-as-Mask 的方法，使多模态大模型具备基于推理的分割能力。LISA 能够处理复杂的自然语言指令并生成细粒度的分割结果，拓宽了分割模型应用的范围和复杂性。

#### 3.3.3  单目深度估计的 VFM

在单目深度估计领域，VFM 也展示了显著的技术优势。ZoeDepth [[212]] 通过结合相对和绝对深度估计方法实现了零-shot 深度估计。ScaleDepth [[213]] 将深度估计分解为两个模块：场景规模预测和相对深度估计，在室内、室外、无约束和未见场景中实现了先进的性能。此外，Depth Anything [[214]] 利用许多未标记的单目图像训练出一种高效且鲁棒的深度估计方法，在零-shot 场景中展示了卓越的性能。Depth Anything V2 [[215]] 对原始模型进行了多项优化，进一步提升了复杂场景中的预测性能，并能够生成高质量的深度图像，细节丰富。 基于多尺度 ViT 架构的 Depth Pro [[216]]，能够快速生成具有高分辨率和高频细节的度量准确深度图像，使其成为处理复杂深度估计任务的有效工具。
