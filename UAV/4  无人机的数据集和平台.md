# 4  无人机的数据集和平台

本节回顾了与无人机研究相关的公开可用数据集和模拟平台，这些是推动基于基础模型（FM）无人机系统综合研究的重要资源。高质量的数据集构成了无人机视觉算法和自主行为学习的基础，通过提供多样化和全面的训练数据。与此同时，三维模拟平台为无人机系统的发展、测试和验证提供了安全和受控的虚拟环境。这些平台能够模拟复杂的场景和环境条件，使研究人员能够以无风险和成本效益高的方式进行实验。

我们提供了一系列开源数据集，这些数据集主要用于无人驾驶飞行器（UAV）系统的开发，所有数据集均已验证为可公开下载。数据集的组织情况见表 [3]、[4]、[5]、[6]、[7]、[8]、[9]。 “年份”列指示每个数据集的最新更新时间；如果没有更新，则列出相关论文的出版年份。“类型”列中的图像和视频默认为 RGB。

这些数据集涵盖多种格式，包括视频、RGB 图像（表格中的默认格式）、LiDAR 点云、红外图像、深度图像和文本数据（如描述或注释）。视频和 RGB 图像是主要的数据类型，而文本数据则较为少见。值得注意的是，一些数据集已更新以包含新功能。例如，EAR 数据集 [[246]] 增加了字幕和问答能力，演变为 CapEAR 数据集 [[247]]，现在适用于视觉问答（VQA）任务。表中列出的多数数据集均来自户外环境，并分为两类：一般领域数据集和特定领域数据集。

表 3:  面向无人机的环境感知与事件识别数据集

|**名称**|**年份**|**类型**|**数量**|
| ------------| ------| ----------------------------------------------------| -----------------------------------------------------------------------------------------------------------------------------------|
|环境感知||||
|空气鱼眼[[248]]|2024|鱼眼图像<br /><br />深度图像<br /><br />点云<br /><br />惯性测量单元 (IMU)|总共超过 26,000 张鱼眼图像。数据以每秒 10 帧的速率收集。[\faExternalLink](https://collaborating.tuhh.de/ilt/airfisheye-dataset)|
|SynDrone[[244]]|2023|图像<br /><br />深度图像<br /><br />点云|包含 72,000 个注释样本，提供 28 种像素级和目标级的注释。[\faExternalLink](https://github.com/LTTM/Syndrone)|
|WildUAV[[249]]|2022|图像<br /><br />视频<br /><br />深度图像<br /><br />元数据|映射图像以 24 位 PNG 文件提供，分辨率为 5280x3956。视频图像以 3840x2160 的分辨率提供为 JPG 文件。详细说明了 16 种可能的类别标签。[\faExternalLink](https://github.com/hrflr/wuav)|
|事件识别||||
|CapERA[[247]]|2023|视频<br /><br />文本|2864 个视频，每个视频配有 5 个描述，总计 14,320 条文本。每个视频持续 5 秒，以每秒 30 帧的速度拍摄，分辨率为 640 × 640 像素。[\faExternalLink](https://github.com/yakoubbazi/CapEra)|
|ERA[[246]]|2020|视频|共有 2,864 个视频，包括灾害事件、交通事故、体育比赛和其他 25 个类别。每个视频为 24 帧/秒，共 5 秒。[\faExternalLink](https://lcmou.github.io/ERA_Dataset)|
|VIRAT[[250]]|2016|视频|25 小时的静态地面视频和 4 小时的动态空中视频。涉及 23 种事件类型。[\faExternalLink](https://viratdata.org/)|

表 4： 面向无人机的目标跟踪数据集

|**名称**|**年份**|**类型**|**数量**|
| --------------------------| ------| --------------------------| ------------------------------------------------------------------------------------------------------------------------------------------|
|WebUAV-3M[[251]]|2024|视频<br /><br />文本<br /><br />音频|4,500 个视频，总计超过 330 万个帧，涵盖 223 个目标类别，提供自然语言和音频描述。[\faExternalLink](https://github.com/983632847/WebUAV-3M)|
|UAVDark135[[252]]|2022|视频|135 个视频序列，超过 125,000 帧手动标注的图像。[\faExternalLink](https://vision4robotics.github.io/project/uavdark135/)|
|DUT-VTUAV[[253]]|2022|RGB-T 图像|近 170 万个对齐良好的可见光-热成像（RGB-T）图像对，包含 500 个序列，揭示 RGB-T 跟踪的强大能力。包括 2 个城市中的 13 个子类和 15 个场景。[\faExternalLink](https://github.com/zhang-pengyu/DUT-VTUAV)|
|TNL2K[[254]]|2022|视频<br /><br />红外视频<br /><br />文本|2000 个视频序列，包括 1,244,340 帧和 663 个单词。[\faExternalLink](https://github.com/wangxiao5791509/TNL2K_evaluation_toolkit)|
|PRAI-1581[[255]]|2020|图像|39,461 张 1581 个身份的人物图像。[\faExternalLink](https://github.com/stormyoung/PRAI-1581)|
|VOT-ST2020/ VOT-RT2020[[256]]|2020|视频|1,000 个序列，长度各异，平均长度约为 100 帧。[\faExternalLink](https://votchallenge.net/vot2020/dataset.html)|
|VOT-LT2020[[256]]|2020|视频|50 个序列，每个序列的长度约为 40,000 帧。[\faExternalLink](https://votchallenge.net/vot2020/dataset.html)|
|VOT-RGBT2020[[256]]|2020|视频<br /><br />红外视频|50 个序列， 每个序列长度约为 40,000 帧。[\faExternalLink](https://votchallenge.net/vot2020/dataset.html)|
|VOT-RGBD2020[[256]]|2020|视频<br /><br />深度图像|80 个序列，总共约 101,956 帧。[\faExternalLink](https://votchallenge.net/vot2020/dataset.html)|
|GOT-10K[[257]]|2019|图像<br /><br />视频|属于 84 个物体类别和 31 个运动类别的 420 个视频剪辑。[\faExternalLink](http://got-10k.aitestunion.com/)|
|DTB70[[258]]|2017|视频|70 个视频序列，每个序列由多个视频帧组成，每帧包含分辨率为 1280x720 像素的 RGB 图像。[\faExternalLink](https://github.com/flyers/drone-tracking)|
|斯坦福无人机[[259]]|2016|视频|19,000 多个目标轨迹，包含 6 种目标，大约 20,000 个目标交互，与环境的 40,000 个目标交互，覆盖大学校园内 100 多个场景。[\faExternalLink](https://cvgl.stanford.edu/projects/uav_data/)|
|COWC[[260]]|2016|图像|标注了 32,716 个独特车辆和 58,247 个非车辆目标，覆盖 6 个不同的地理区域。[\faExternalLink](https://gdo152.llnl.gov/cowc/)|

### 4.1  一般领域数据集

一般领域数据集旨在满足广泛场景的需求，进一步根据特定任务进行分类，包括环境感知、事件识别、目标跟踪、动作识别和导航。在环境感知类别中，我们专注于诸如目标检测、分割和深度估计等任务。虽然事件识别、目标跟踪和动作识别也可以视为环境感知的一部分，但为了更清晰地呈现数据集，我们将它们单独列出。

#### 4.1.1  环境感知

这一部分展示了主要用于目标检测、分割和深度估计的数据集，如表 [3] 所示。例如，AirFisheye 数据集 [[248]] 专门设计用于 UAV 捕获的复杂城市环境中的目标检测、分割和深度估计等任务。它的多模态数据包括视觉、热成像和激光雷达，为分析这些复杂城市环境中的场景提供了全面的信息。SynDrone 数据集 [[244]] 是一个使用 Carla 生成的大规模合成数据集，旨在用于城市环境中的检测和分割任务。WildUAV 数据集 [[249]] 提供高分辨率 RGB 图像和深度真值数据，专注于单目视觉深度估计，同时支持在复杂环境中的精确无人机飞行控制。

#### 4.1.2  事件识别

事件识别的典型数据集列于表 [3] 中。EAR 数据集 [[246]] 作为事件识别的视频基准，涵盖了 25 个事件类别，包括地震后、洪水、火灾、滑坡、泥石流、交通碰撞、交通拥堵、收割、耕作、建设、警方追捕、冲突、各种体育活动（如棒球、篮球、骑行）和社交活动（如聚会、音乐会、抗议、宗教活动）。该数据集包含 2864 个视频，每个视频时长为 5 秒，通过将“无人机”和“UAV”作为搜索关键词从 YouTube 收集而来。类似地，VIRAT 数据集 [[250]] 专注于监控视频中的事件识别，包括交通事故和人群聚集等事件。尽管并非由 UAV 捕获，VIRAT 数据集提供了类似的空中视角，使其与基于无人机的场景分析相关联。这些数据集为事件检测和场景理解的研究提供了宝贵的资源，特别是在将 UAV 应用与 LLMs 整合的背景下。

#### 4.1.3  对象跟踪

对象跟踪任务依赖于多样化的数据集，以推动各个领域的研究。表 [4] 列出了此任务的典型数据集。WebUAV-3M 数据集 [[251]] 是一个大规模的无人机对象跟踪基准，包含 4,500 个视频和 233 个对象类别。它为一般场景中的无人机跟踪提供了坚实的基础，并包括多模态数据，例如音频和自然语言描述，使得多模态无人机跟踪方法的探索成为可能。TNL2K 数据集 [[254]] 专注于自然语言引导的对象跟踪，包含 2,000 个带有边界框标注和详细自然语言描述的视频序列，这些描述捕捉了目标对象的类别、形状、属性、特征和空间位置。为了应对挑战性的场景，TNL2K 包含对抗样本和外观变化显著的序列，提供 RGB 和红外模态以支持跨模态跟踪研究。 VOT2020 数据集 [[256]] 提供了五个针对特定任务的专业数据集的全面集合：短期跟踪、实时跟踪、长期跟踪、热成像跟踪和深度跟踪。这些数据集共同应对了广泛的跟踪挑战，促进了不同跟踪范式的创新。

#### 4.1.4  动作识别

使无人机理解人类动作并通过手势解释指令是一个关键的研究领域。表 [5] 列出了用于动作识别的无人机相关数据集。Aeriform In-Action 数据集 [[261]] 针对空中视频中的人类动作识别，包含 32 个高分辨率视频，跨越 13 个动作类别。该数据集专门设计用于应对空中监控中与动作识别相关的独特挑战。MEVA 数据集 [[262]] 提供了一个大规模、多视角、多模态的数据集，包含 9300 小时由无人机和地面摄像机拍摄的连续视频。它涵盖 37 个活动类别并促进先进任务，例如多视角活动检测。此外，UAV-Human 数据集 [[79]] 提供了 67,428 个多模态视频序列，包括 119 个对象用于动作识别。除了动作识别外，它还支持姿态估计和人员重识别等任务。 该数据集涵盖了多样的背景、光照条件和环境，为基于无人机的人类行为分析提供了全面的基准。

表 5： 面向无人机的行动识别数据集

|**名称**|**年份**|**类型**|**数量**|
| ----------------------| ------| --------------------------------------------------------------| -----------------------------------------------------------------------------------------------------------------------------------------------------------------|
|气体的<br /><br />行动中的[[261]]|2023|视频|32 个视频，13 种动作，55,477 帧，40,000 个标注。[\faExternalLink](https://surbhi-31.github.io/Aeriform-in-action/)|
|MEVA[[262]]|2021|视频<br /><br />红外视频<br /><br />GPS<br /><br />点云|总共 9300 小时的视频，144 小时的活动记录，37 种活动类型，超过 270 万个 GPS 轨迹点。[\faExternalLink](https://mevadata.org/)|
|无人机-人类[[79]]|2021|视频<br /><br />夜视视频<br /><br />鱼眼视频<br /><br />深度视频<br /><br />红外视频<br /><br />骨架|67,428 个视频（155 种动作，119 个主体），22,476 帧标注关键点（17 个关键点），41,290 帧人物再识别（1,144 个身份），22,263 帧属性识别（例如性别、帽子、背包等）。[\faExternalLink](https://github.com/SUTDCV/UAV-Human)|
|MOD20[[263]]|2020|视频|20 种动作，2,324 个视频，503,086 帧。[\faExternalLink](https://asankagp.github.io/mod20/)|
|NEC-无人机[[264]]|2020|视频|包含 256 分钟的行动视频的 5,250 个视频，涉及 19 名演员和 16 个行动类别[\faExternalLink](https://www.nec-labs.com/research/media-analytics/projects/unsupervised-semi-supervised-domain-adaptation-for-action-recognition-from-drones/)|
|无人机行动[[265]]|2019|视频|240 个高清录像，66,919 帧，13 种动作。[\faExternalLink](https://asankagp.github.io/droneaction/)|
|无人机手势[[266]]|2019|视频|119 个录像，37,151 帧，13 种手势，10 名演员。[\faExternalLink](https://asankagp.github.io/uavgesture/)|

#### 4.1.5  导航与定位

表 [6] 展示了面向无人机的导航和定位数据集。CityNav 数据集 [[267]] 是一个专为语言指导的空中导航任务而设计的数据集，旨在帮助无人机使用自然语言指令在城市规模的三维环境中导航。该数据集包含 32,000 个任务，提供广泛的地理信息和详细的城市环境模型。AerialVLN 数据集 [[268]] 关注通过视觉和语言线索的整合进行无人机导航，使无人机能够基于自然语言命令在复杂环境中执行飞行任务，从而增强其在动态环境中的适应能力。VIGOR 数据集 [[269]] 提供了一个跨视角图像定位数据集，促进了从不同视角对无人机的精确地理定位，提高了在复杂地理环境中图像匹配和位置校准的精度。 University-1652 数据集 [[270]] 作为跨视角地理定位的基准，填补了地面视角和卫星视角之间的视觉差距，采用了无人机视角图像。它包括来自 1,652 所大学的合成无人机、卫星和地面相机的配对图像，支持两个任务：无人机视角目标定位和无人机导航。

表 6： 针对无人机的导航与定位数据集

|**名称**|**年份**|**类型**|**数量**|
| -------------------| ------| --------------------------| --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
|CityNav[[267]]|2024|图像<br /><br />文本|32,000 个自然语言描述和配套轨迹。[\faExternalLink](https://water-cookie.github.io/city-nav-proj/)|
|CNER-UAV[[271]]|2024|文本|12,000 个标记样本包含 5 种地址标签（例如，建筑物、单元、楼层、房间等）。[\faExternalLink](https://github.com/zhhvvv/CNER-UAV)|
|空中视觉导航[[268]]|2023|模拟器路径<br /><br />文本|它包含25个城市级场景，包括城市地区、工厂、公园和村庄。共有8,446条路径。每条路径配有3个自然语言描述，总计25,338条指令。|
|稠密无人机[[272]]|2023|图像|训练集：6,768 张无人机图像，13,536 张卫星图像。测试集：2,331 张无人机查询图像和 4,662 张卫星图像。[\faExternalLink](https://github.com/Dmmm1997/DenseUAV)|
|map2seq[[273]]|2022|图像<br /><br />文本<br /><br />地图路径|29,641 张全景图像，7,672 条导航指令文本。[\faExternalLink](https://map2seq.schumann.pub/dataset/download/)|
|VIGOR[[269]]|2021|图像|90,618 张航拍图像，238,696 个街景全景。[\faExternalLink](https://github.com/Jeff-Zilence/VIGOR)|
|University-1652[[270]]|2020|图像|收集了 1,652 座大学建筑，涉及 72 所大学，50,218 张训练图像，37,855 张无人机视角查询图像，701 张卫星视角查询图像，以及额外的 21,099 张普通视角和 5,580 张街景视角图像用于训练。[\faExternalLink](https://github.com/layumi/University1652-Baseline)|

### 4.2  领域特定数据集

与通用领域数据集相比，领域特定数据集旨在针对特定应用进行定制，并根据其所涉及的具体领域进行分类，包括交通、遥感、农业、工业应用、应急响应、军事行动和野生动物。

#### 4.2.1  交通运输

交通场景是无人机数据集中最常见的场景之一，本部分重点介绍专为交通监测以及车辆和行人检测任务而设计的数据集（见表 [7]），这些任务是无人机技术的关键应用。TrafficNight 数据集 [[274]] 是一种用于夜间车辆监测的航空多模态数据集，旨在解决现有航空数据集在光照条件和车辆类型代表性方面的局限性。该数据集结合了垂直 RGB 和热红外成像技术，覆盖多种场景，包括拥有大量半挂车的场景，并提供专业注释。它还包括相应的 HD-MAP 数据，以便进行多车辆追踪。VisDrone 数据集 [[275]] 是一个大规模基准，支持图像和视频中的目标检测，以及单一和多目标追踪。该数据集收集自中国 14 个城市，具有高度的多样性和挑战性场景，使其非常适合在复杂的城市和郊区环境中评估算法。 CADP 数据集 [[276]] 强调交通事故分析，通过使用 CCTV 交通监控视频提高小物体检测准确性（例如行人）。它整合了上下文挖掘技术和基于 LSTM 的架构用于事故预测。CARPK 数据集 [[277]] 引入了一种新颖的停车场车辆计数方法，采用空间正则化区域建议网络（LPN）。该数据集包含超过 90,000 辆车辆的高分辨率无人机影像，增强了物体检测和计数性能。iSAID 数据集 [[278]] 为实例分割任务提供高质量注释，包括 15 个类别下的 655,451 个标注实例，从而支持无人机应用中的准确物体检测和场景分析。这些数据集共同推动了车辆检测、物体跟踪、交通监控和无人机自主导航的研究，为智能交通、无人机巡逻和配送系统的应用提供了强大的数据资源。

表 7： 面向无人机的交通数据集

|**名称**|**年份**|**类型**|**数量**|
| -------------------------| ------| ----------------------------------------------| ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
|交通夜晚[[274]]|2024|图像<br /><br />红外图像<br /><br />视频<br /><br />红外视频<br /><br />地图|该数据集由 2200 对注释的热红外图像和 sRGB 图像数据以及来自 7 个交通场景的视频数据组成，总时长约为 240 分钟。每个场景包括一幅高精度地图，提供详细的布局和拓扑信息。[\faExternalLink](https://github.com/AIMSPolyU/TrafficNight)|
|VisDrone[[275]]|2022|图像<br /><br />视频|263 个视频，179,264 帧。10,209 张静态图片。超过 2,500,000 个目标实例注释。数据覆盖 14 个不同城市，涵盖广泛的天气和光照条件。[\faExternalLink](http://aiskyeye.com/home/)|
|ITCVD[[279]]|2020|图像|共收集了 173 幅航空图像，其中训练集中有 135 幅图像，包含 23,543 辆车辆，测试集中有 38 幅图像，包含 5,545 辆车辆。图像之间有 60%的区域重叠，而训练集与测试集之间没有重叠。[\faExternalLink](https://research.utwente.nl/en/datasets/itcvd-dataset)|
|UAVid[[280]]|2020|图像<br /><br />视频|30 个视频，300 幅图像，8 个语义类别标注。[\faExternalLink](https://uavid.nl/)|
|AU-AIR[[281]]|2020|视频<br /><br />GPS<br /><br />高度<br /><br />IMU<br /><br />速度|32,823 帧视频，分辨率 1920x1080，30 帧每秒，划分为 30,000 个训练验证样本和 2,823 个测试样本。8 段视频的总时长约为 2 小时，总实例数为 132,034，分布在 8 个类别中。[\faExternalLink](https://bozcani.github.io/auairdataset)|
|iSAID[[278]]|2020|图像|总图像数：2,806。实例总数：655,451。测试集：935 张图像（未公开标注，用于评估服务器）。[\faExternalLink](https://captain-whu.github.io/iSAID/)|
|CARPK[[277]]|2018|图像|1448 张图像，约 89,777 辆车辆，提供框注释。[\faExternalLink](https://lafi.github.io/LPN/)|
|高维（highD）[[282]]|2018|视频<br /><br />轨迹|16.5 小时，110,000 辆车，5,600 次车道变换，45,000 公里，总计约 447 小时的车辆行驶数据；4 个预定义驾驶行为标签。[\faExternalLink](https://levelxdata.com/highd-dataset/)|
|无人机数据集（UAVDT）[[283]]|2018|视频<br /><br />天气<br /><br />高度<br /><br />摄像机角度|100 个视频，约 80,000 帧，30 帧每秒，包含 841,500 个目标框，涵盖 2,700 个目标。[\faExternalLink](https://sites.google.com/view/grli-uavdt/)|
|CADP[[276]]|2016|视频|总计 5.24 小时，1,416 个交通事故视频片段，205 个全时空注释视频。[\faExternalLink](https://ankitshah009.github.io/accident_forecasting_traffic_camera)|
|VEDAI[[284]]|2016|图像|1,210 张图片（1024 × 1024 和 512 × 512 像素），9 种车辆，总计包含约 6,650 个目标。[\faExternalLink](https://downloads.greyc.fr/vedai)|

#### 4.2.2  遥感

在遥感领域，多个创新数据集，如表 [8] 所示，为目标检测、分类、定位和图像分析等任务提供了实质性的支持 [[14]]。xView 数据集 [[285]] 是一种大规模卫星图像数据集，包含超过一百万个注释，涵盖多个物体类别，使其特别适合于目标检测和图像分割任务，尤其是在复杂背景和具有挑战性的环境中。DOTA 数据集 [[286]] 专注于高分辨率航空图像中的目标检测，涵盖多个物体类别，如飞机、车辆和建筑物，适合于复杂场景中的多目标检测和分类任务。RSICD 数据集 [[287]] 主要用于遥感图像中的场景分类任务，并支持语言描述生成，提供了一个标准化的基准，促进了图像理解和自动注释技术的研究。 RemoteCLIP [[288]] 引入了一种遥感视觉语言模型，通过自监督学习和掩蔽图像建模增强遥感图像的语义分析和图像检索，推动了无人机在遥感数据分析中的应用。

表 8： 针对遥感的无人机数据集

|**名称**|**年份**|**类型**|**数量**|
| -------------------| ------| --------------| ---------------------------------------------------------------------------------------------------------------------------------------------------|
|RET-3[[288]]|2024|图像<br /><br />文本|大约 13,000 个样本，包括 RSICD、RSITMD 和 UCM。[\faExternalLink](https://github.com/ChenDelong1999/RemoteCLIP?utm_source=chatgpt.com)|
|DET-10[[288]]|2024|图像|在目标检测数据集中，每张图像中的物体数量范围从 1 到 70，总计约 80,000 个样本。[\faExternalLink](https://github.com/ChenDelong1999/RemoteCLIP?utm_source=chatgpt.com)|
|SEG-4[[288]]|2024|图像|该分段数据集覆盖不同区域和分辨率，共计约 72,000 个样本。[\faExternalLink](https://github.com/ChenDelong1999/RemoteCLIP?utm_source=chatgpt.com)|
|DIOR[[289]]|2020|图像|23,463 张图像，包含 192,472 个目标实例，覆盖 20 个类别，包括飞机、车辆、船只、桥梁等，每个类别约包含 1,200 个实例。[\faExternalLink](http://www.escience.cn/people/gongcheng/DIOR.html)|
|TGRS-HRRSD[[290]]|2019|图像|总图像数：21,761。包含 13 个类别，包括飞机、车辆、桥梁等。目标总数约为 53,000 个。[\faExternalLink](https://github.com/CrazyStoneonRoad/TGRS-HRRSD-Dataset)|
|xView[[285]]|2018|图像|目标超过 100 万个，包含 60 个类别，包括车辆、建筑、设施、船只等，分为七个父类别和若干子类别。[\faExternalLink](https://xviewdataset.org/)|
|DOTA[[286]]|2018|图像|2806 张图像，188,282 个目标，15 个类别。[\faExternalLink](https://captain-whu.github.io/DOTA/)|
|RSICD[[287]]|2018|图像<br /><br />文本|10,921 张图像，54,605 个描述性句子。[\faExternalLink](https://github.com/201528014227051/RSICD_optimal)|
|HRSC2016[[291]]|2017|图像|包含 3,433 个实例，总计 1,061 张图像，其中包括 70 张纯海洋图像和 991 张包含混合陆海区域的图像。标记的船只目标有 2,876 个。未标记的图像有 610 张。[\faExternalLink](http://www.escience.cn/people/liuzikun/DataSet.html)|
|RSOD[[292]]|2017|图像|包含 4 种类型的目标（坦克、飞机、高架桥、游乐场），有 12,000 个正样本和 48,000 个负样本。[\faExternalLink](https://github.com/RSIA-LIESMARS-WHU/RSOD-Dataset-)|
|NWPU-RESISC45[[293]]|2017|图像|总计 31,500 张图像，涵盖 45 个场景类别，每个类别 700 张图像，分辨率为 256 × 256 像素，空间分辨率从 0.2 米到 30 米。[\faExternalLink](http://pan.baidu.com/s/1mifR6tU)|
|NWPU VHR-10<br /><br />[[294]]|2014|图像|800 张高分辨率图像，其中 650 张包含目标，150 张为背景图像，覆盖 10 个类别（如飞机、船只、桥梁等），总计超过 3,000 个目标。[\faExternalLink](https://github.com/Gaoshuaikun/NWPU-VHR-10)|

#### 4.2.3  农业

农业部分总结了过去两年中仅公开可用的数据集，如表 [9] 所示，因为几篇综述文章已覆盖 2023 年前的数据集。农业数据集通常用于目标检测，以识别杂草、入侵植物或植物疾病和害虫，而语义分割通常用于田地划分。Avo-AirDB 数据集 [[295]] 专门用于农业图像分割和分类，提供高分辨率的鳄梨作物图像，以支持在精准农业中的植物识别和健康监测。CoFly-WeedDB 数据集 [[296]] 由 201 幅航空图像组成，捕捉到三种干扰棉花作物的杂草，以及相应的注释图像。WEED-2C 数据集 [[297]] 专注于训练无人机图像，以在大豆田中识别杂草物种，自动识别两种杂草物种。

#### 4.2.4  行业

使用无人机影像进行工业检查，特别是在基础设施维护方面，变得越来越重要。表 [9] 列出了几个典型数据集。UAPD 数据集 [[298]] 专注于通过无人机影像和 YOLO 架构检测沥青路面裂缝，旨在通过自动化裂缝检测提高公路和高速公路的维护效率。InsPLAD 数据集 [[299]] 专门为电力线路资产检测而设计，包含集中于基础设施（如电力线路、塔和绝缘子的无人机影像）。通过提供在多种环境条件下的影像，该数据集支持开发自动检查系统，以识别电力设备中的损坏或老化，从而提高电力线路检查的效率和准确性。

#### 4.2.5  紧急响应

这些数据集如表 [9] 所示，通常用于增强无人机在灾害救援场景中的视觉理解能力，特别是在灾后场景分析、灾区监测、环境评估和救援操作中。它们促进了快速图像识别、目标检测和场景理解任务。航空合成孔径雷达数据集 [[300]] 探讨了无人机在自然灾害监测和搜救操作中的应用，强调了在灾区快速部署和自主管理无人机的潜力。AFID 数据集 [[301]] 提供了用于水道监测和灾害预警的航空图像，支持深度语义分割模型的训练。FloodNet 数据集 [[302]] 提供了高分辨率航空图像，用于灾后场景理解，主要旨在协助灾后评估和紧急救援操作。 通过利用这些数据集，研究人员可以显著提高在灾难响应中的图像分析能力，并推动无人机技术在灾难救援工作中的实际应用。

#### 4.2.6  军事

MOCO 数据集 [[303]] 是为军事图像字幕生成（MilitIC）任务而设计的，该任务专注于从低空无人机（UAV）和无人地面车辆（UGV）在军事背景下捕获的图像中生成文本信息。该数据集包含一个训练集，包括 7192 张图像和 35960 个字幕，以及一个测试集，包含 257 张图像和 1285 个字幕。军事图像字幕生成作为一种视觉-语言学习任务，旨在自动生成军事图像的描述性字幕，从而增强态势感知并支持决策制定。通过将图像数据与文本描述结合，这种方法提高了军事领域的情报能力和作战效率。

#### 4.2.7  野生动物

WAID [[304]] 是一个大规模、多类别、高质量的数据集，专门设计用于支持无人机在野生动物监测中的应用。该数据集包括 14,375 幅在各种环境条件下拍摄的无人机图像，涵盖六种野生动物和多种栖息地。

表 9： 面向农业、工业、应急响应、军事和野生动物的无人机相关数据集

|**名称**|**年份**|**类型**|**数量**|
| --------------------| ------| ------------------| --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
|农业||||
|WEED-2C[[297]]|2024|图像|包含 4,129 个标记样本，涵盖 2 种杂草物种。[\faExternalLink](https://github.com/EvertonTetila/WEED2C-Dataset/?tab=readme-ov-file)|
|CoFly-WeedDB[[296]]|2023|图像<br /><br />健康数据|由 201 张空中图像组成，捕获了 3 种扰动行作物（棉花）及其相应的注释图像中的不同杂草类型。[\faExternalLink](https://github.com/CoFly-Project/CoFly-WeedDB/blob/main/README.md?utm_source=chatgpt.com)|
|Avo-AirDB[[295]]|2022|图像|984 张高分辨率 RGB 图像（5472 × 3648 像素），其中 93 张具有详细的多边形注释，分为 3 至 4 个类别（小型、中型、大型和背景）。[\faExternalLink](https://github.com/LCSkhalid/Avo-AirDB)|
|行业||||
|UAPD[[298]]|2021|图像|原始数据中有 2,401 张裂缝图像，经过数据增强后共有 4,479 张裂缝图像。[\faExternalLink](https://github.com/tantantetetao/UAPD-Pavement-Distress-Dataset)|
|InsPLAD[[299]]|2023|图像|包含 17 类电力资产的 10,607 张无人机图像，总共有 28,933 个标注实例，5 种资产的缺陷标注，总共 402 个缺陷样本被分类为 6 种缺陷类型。[\faExternalLink](https://github.com/andreluizbvs/InsPLAD/)|
|紧急响应||||
|FloodNet[[302]]|2021|图像<br /><br />文本|整个数据集包含 2,343 张图像，分为训练集（60%）、验证集（20%）和测试集（20%）。语义分割标签包括：背景、建筑被淹、建筑未被淹、道路被淹、道路未被淹、水、树木、车辆、池塘、草地。[\faExternalLink](https://github.com/BinaLab/FloodNet-Supervised_v1.0)|
|AFID[[301]]|2023|图像|总共有 816 幅分辨率为 2720 × 1536 和 2560 × 1440 的图像。包含 8 个语义分割类别。[\faExternalLink](https://purr.purdue.edu/publications/4105/1)|
|空中合成孔径雷达[[300]]|2020|图像|2,000 幅图像包含 30,000 个动作实例，覆盖多种人类行为。[\faExternalLink](https://www.leadingindia.ai/data-set)|
|军事||||
|MOCO[[303]]|2024|图像<br /><br />文本|7,449 张图片，37,245 条标题。[\faExternalLink](https://github.com/Panlizhi/MOCO)|
|野生动物||||
|WAID[[304]]|2023|图像|14,375 张无人机图像涵盖 6 种野生动物和多种环境类型。[\faExternalLink](https://github.com/xiaohuicui/WAID)|

### 4.3 3D 模拟平台

三维仿真平台在无人机的发展和应用中发挥着至关重要的作用，为智能无人机训练提供安全、可控和多样化的测试场景，这些场景位于高度仿真的虚拟环境中。这些环境涵盖复杂的条件，如变化的天气、光照、风速、地形和障碍物。这类平台能够生成大规模、准确标注的多模态数据集，用于训练和验证。此外，仿真平台支持多机协作任务的建模，评估无人机在共享空间内的协作能力、通信和避障策略。这有效降低了与实际测试相关的风险和成本。硬件在环（HIL）仿真进一步将虚拟测试与真实硬件相结合，帮助识别潜在问题并验证系统可靠性。总之，三维仿真平台在智能训练、数据集生成、协作任务执行和硬件验证方面起到了关键作用，显著加速了无人机技术的发展和部署。

#### 4.3.1 **AirSim**

AirSim [[305]] 是由微软开发的开源跨平台模拟器，专为无人机、自动驾驶车辆及其他自主系统的研究与开发而设计。该平台基于虚幻引擎构建，提供高度逼真的物理模拟环境和视觉效果，使用户能够在虚拟场景中测试和验证算法的性能。AirSim 支持各种设备和传感器的模拟，包括摄像头、激光雷达、惯性测量单元（IMU）、全球定位系统（GPS）等，同时通过强大的 API 提供对环境和车辆的全面控制。开发者可以使用 Python 和 C++扩展该平台，实现机器学习、计算机视觉和机器人等领域前沿技术的集成。除了模拟无人机和地面车辆外，该平台还可以建模复杂的动态场景，包括天气变化、碰撞检测和物理交互，帮助用户在安全可控的虚拟环境中加速原型验证和算法优化。

#### 4.3.2 **Carla**

CARLA [[306]] 是一个基于虚幻引擎的开源自动驾驶模拟平台，广泛用于智能系统算法的开发、训练和验证。其高度真实的模拟环境支持复杂的城市场景，包括道路网络、动态交通、行人行为以及多样的天气和光照条件，为感知、定位、规划和控制算法提供了一个虚拟测试场。CARLA 支持多种传感器的模拟，如摄像头、激光雷达、雷达、惯性测量单元和全球定位系统，并允许用户访问其 Python 或 C++ API，以及支持 ROS 的接口，使研究人员能够快速开发和测试导航、避障、路径规划和环境感知的算法。此外，CARLA 提供数据录制和回放功能，支持多智能体任务，并集成强化学习应用，为低空物流、监控和无人机巡逻等场景中的算法开发提供安全、高效和可重复的测试平台。

#### 4.3.3 **NVIDIA Isaac Sim**

NVIDIA Isaac Sim [[307]] 是一个基于物理的机器人模拟平台，建立在 NVIDIA Omniverse 平台之上，为机器人和自主系统的开发、测试和验证提供高精度的虚拟环境。该平台利用 NVIDIA 强大的 GPU 加速和物理引擎技术，包括 PhysX 和 RTX 实时渲染，呈现高度逼真的模拟场景，具有准确的物理交互、光照效果和多传感器数据生成。Isaac Sim 提供广泛的工具和插件，允许与各种机器人框架集成，并支持从感知、运动规划到控制算法的完整开发过程。除了在传统机器人领域的应用，Isaac Sim 还可以扩展到无人机领域，通过灵活的环境配置、传感器模拟（包括摄像头、激光雷达、惯性测量单元和 GPS）以及复杂的动力学建模，支持无人机导航、障碍避让、目标跟踪和多智能体协作任务。 该平台结合了模拟强化学习能力、数据收集功能和数字双胞胎支持，以应对现实世界场景，从而加速无人机在物流、环境监测和灾难响应等领域的算法开发，同时为研究人员和开发者提供高效、安全和可扩展的测试环境。

#### 4.3.4  空中虚拟导航模拟器

AerialVLN 仿真器 [[268]] 是一个高保真虚拟仿真平台，专门用于无人机代理的研究。它结合了虚幻引擎 4 和微软 AirSim 技术，真实地模拟典型的 3D 城市环境，包括上海、深圳等城市、校园和居民区，覆盖范围从 30 公顷到 3,700 公顷。该平台支持多样的环境设置，包括昼夜不同的光照条件、天气模式如晴天、阴天和小雪，以及季节变化，使无人机代理能够在与现实条件密切相关的环境中进行训练。平台配备前、后、左、右和顶部的多视角摄像头，能够生成 RGB 图像、深度图像和目标分割图等高分辨率数据，为场景理解和空间建模提供丰富的视觉输入。 此外，AerialVLN 模拟器支持无人机的动态飞行操作，提供对其三维位置、方向和速度的精确控制，同时允许执行复杂的机动，如转弯、爬升和避障，确保飞行动作的平滑和灵活。基于“真实到模拟再到真实”的设计理念，该平台显著缩小了虚拟环境与现实世界应用之间的差距，使其特别适合于核心无人机任务的研究和优化，如场景感知、空间推理、路径规划和运动决策。

#### 4.3.5  具身城市

具体现实城市 [[308]] 是一个先进的高保真 3D 城市仿真平台，专为具体现智能的评估和开发而设计。其核心特征是基于现实世界城市区域（如北京的商业区）构建的逼真虚拟环境，包括高度详尽的建筑模型、街道网络以及行人和车辆交通的动态仿真。该平台以虚幻引擎为技术基础，将历史数据与仿真算法相结合，为各种具体现代理（如无人机和地面车辆）提供持续的感知和交互能力。通过集成 AirSim 接口，它支持多模态输入和输出，包括 RGB 图像、深度图像、激光雷达、GPS 和 IMU 数据，便于在仿真中进行运动控制和环境探索。设计涵盖五个任务领域：场景理解、问答、对话、视觉语言导航和任务规划。 通过一个易于使用的 Python SDK 和一个在线平台，用户可以方便地远程访问和测试多种代理行为，同时支持多达八个代理的实时操作。
